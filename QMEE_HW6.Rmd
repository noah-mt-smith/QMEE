# Noah Smith QMEE homework 6

### Question 1a:

![](hw6_q1a_figure.jpg)

### Question 1b:

My two main response variables (proportion of funds allocated to winner and proportion of coaching hours allocated to winner) are continuous variables on the ratio scale. This means that, if participant A allocates 0.8 of their funds to the winner and participant B allocates 0.1 of their funds to the winner, participant A has allocated 8 times as much. And if participant C allocated 0.4 of their funds to the winner, they allocated half of what participant A allocated and 4 times what participant B allocated. I can therefore take the mean of my response variable and compare it to my null of 0.5, as the mean for continuous ratio variables is a meaningful measure. If my response variables had been nominal (e.g., "all funds to winner" as 1, and "no funds to winner" as 0), then a mean measure would be meaningless. Similarly, if my response variables were ordinal instead of continuous ratio variables, then I also wouldn't be able to compare my mean to my null--in the cases of nominal or ordinal data, it would make more sense to use the median or mode response. In terms of the inferential approach, I have tried several up to this point, including a one-sample t-test, a separate linear model that includes my various predictor variables (which violates several important assumptions), and when we learn permutations I am interested to see if they are appropriate for my data, given their lack of reliance on some assumptions that my lm is probably violating. I had asked Ben if a one-sample permutation test would be feasible, and he seemed to indicate that it would not be, but I am interested to learn more about this next week.

### Question 2a:

For your continuous response variable, and focusing on one (or at most two) predictors ( continuous or factors) you will be using from your dataset for this class, provide what measures of effect you plan to use. Please explain your rationale fully, including any historical precedent in your field (what is typically used in studies similar to yours), and why you do (or do not) agree with the historical precedent.

For both of my continuous response variables (proportion of funds allocated to winner and proportion of coaching hours allocated to the winner), I will be using Cohen's d. Although Nakagawa & Cuthill (2007) note that Hedges' G provides a "correction" to Cohen's d, but that Hedges' G is most useful for investigations with fairly low sample sizes. Underpowered studies/studies with small sample sizes are more prone to generating inflated effect sizes, and Hedges' G remedies that problem. However, since my study is appropriately powered (having 170 individuals, or about 85 observations for the two competitive contexts, and it met the threshold outlined in an a priori power analysis), I feel that using Cohen's d is appropriate here. In particular, I will calculate a standardized effect comparing the mean proportion of funds and coaching hours allocated to the winner to my null of 0.5.

The historical context of Cohen's d is somewhat dicey, as it has (potentially) contributed to the replication crisis in experimental psychology and sociology. For several decades, underpowered samples in experimental psychology generated deflated p-values and inflated effect sizes, the latter of which were often calculated with Cohen's d (either that, or effect size estimates were completely omitted) (Nakagawa and Cuthill, 2007; Simmons et al., 2011). Tiny p-values and inflated effect sizes have fostered confusion and frustration among modern experimental psychologists and the general public. For example, many of the results on subconscious priming or unconscious biases in Daniel Kahneman's "Thinking Fast and Slow" does not replicate, despite low p-values and promising effect sizes. Although Kahneman has since addressed these issues (Yong, 2012), I do not agree with the historical precedent for using Cohen's d, as it often didn't include pondering a typical effect size in the field or the power required to reliably detect the effect of study. The ultimate impact and reach of these inflated effects highlights the importance of developing a proper philosophy around analysis, interpretation, and reporting of effect sizes.

Nakagawa, S., & Cuthill, I. C. (2007). Effect size, confidence interval and statistical significance: a practical guide for biologists. *Biological reviews*, *82*(4), 591-605.

Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. *Psychological science,* 22(11), 1359-1366.

Yong, E. (2012). Nobel laureate challenges psychologists to clean up their act. Nature, 490, 7418.
